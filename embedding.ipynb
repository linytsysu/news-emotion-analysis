{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_client = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/pandas/compat/__init__.py:84: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = pd.read_csv('./data/Train_Dataset.csv')\n",
    "label_df = pd.read_csv('./data/Train_Dataset_Label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = content_df.merge(label_df, on='id')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=512\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "embedded = []\n",
    "for text in df.content.values:\n",
    "    embedded.append(bert_client.encode([text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7265"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = np.array(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7265, 1, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7265, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = []\n",
    "for i in range(embedded.shape[0]):\n",
    "    word_vector.append(embedded[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7265, 768)\n"
     ]
    }
   ],
   "source": [
    "word_vector = np.array(word_vector)\n",
    "print(word_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(word_vector).to_csv('./train_word_vector.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_vector\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (7265, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-306249bc547f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape (7265, 3)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "y = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(768,)),\n",
    "        tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "model.compile(\n",
    "    tf.keras.optimizers.Adam(lr=LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7265/7265 [==============================] - 0s 31us/sample - loss: 0.4222 - acc: 0.8307\n",
      "Epoch 2/50\n",
      "7265/7265 [==============================] - 0s 26us/sample - loss: 0.4208 - acc: 0.8260\n",
      "Epoch 3/50\n",
      "7265/7265 [==============================] - 0s 27us/sample - loss: 0.4126 - acc: 0.8319\n",
      "Epoch 4/50\n",
      "7265/7265 [==============================] - 0s 26us/sample - loss: 0.4101 - acc: 0.8369\n",
      "Epoch 5/50\n",
      "7265/7265 [==============================] - 0s 30us/sample - loss: 0.4074 - acc: 0.8351\n",
      "Epoch 6/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.3985 - acc: 0.8398\n",
      "Epoch 7/50\n",
      "7265/7265 [==============================] - 0s 26us/sample - loss: 0.3998 - acc: 0.8409\n",
      "Epoch 8/50\n",
      "7265/7265 [==============================] - 0s 26us/sample - loss: 0.3919 - acc: 0.8421\n",
      "Epoch 9/50\n",
      "7265/7265 [==============================] - 0s 30us/sample - loss: 0.3881 - acc: 0.8449\n",
      "Epoch 10/50\n",
      "7265/7265 [==============================] - 0s 29us/sample - loss: 0.3821 - acc: 0.8507\n",
      "Epoch 11/50\n",
      "7265/7265 [==============================] - 0s 34us/sample - loss: 0.3825 - acc: 0.8478\n",
      "Epoch 12/50\n",
      "7265/7265 [==============================] - 0s 29us/sample - loss: 0.3752 - acc: 0.8542\n",
      "Epoch 13/50\n",
      "7265/7265 [==============================] - 0s 27us/sample - loss: 0.3719 - acc: 0.8527\n",
      "Epoch 14/50\n",
      "7265/7265 [==============================] - 0s 26us/sample - loss: 0.3690 - acc: 0.8527\n",
      "Epoch 15/50\n",
      "7265/7265 [==============================] - 0s 27us/sample - loss: 0.3595 - acc: 0.8582\n",
      "Epoch 16/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.3564 - acc: 0.8604\n",
      "Epoch 17/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.3526 - acc: 0.8626\n",
      "Epoch 18/50\n",
      "7265/7265 [==============================] - 0s 32us/sample - loss: 0.3499 - acc: 0.8591\n",
      "Epoch 19/50\n",
      "7265/7265 [==============================] - 0s 29us/sample - loss: 0.3504 - acc: 0.8602\n",
      "Epoch 20/50\n",
      "7265/7265 [==============================] - 0s 30us/sample - loss: 0.3418 - acc: 0.8657\n",
      "Epoch 21/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.3349 - acc: 0.8690\n",
      "Epoch 22/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.3314 - acc: 0.8702\n",
      "Epoch 23/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.3296 - acc: 0.8728\n",
      "Epoch 24/50\n",
      "7265/7265 [==============================] - 0s 31us/sample - loss: 0.3242 - acc: 0.8732\n",
      "Epoch 25/50\n",
      "7265/7265 [==============================] - 0s 33us/sample - loss: 0.3202 - acc: 0.8760\n",
      "Epoch 26/50\n",
      "7265/7265 [==============================] - 0s 32us/sample - loss: 0.3156 - acc: 0.8764\n",
      "Epoch 27/50\n",
      "7265/7265 [==============================] - 0s 29us/sample - loss: 0.3092 - acc: 0.8793\n",
      "Epoch 28/50\n",
      "7265/7265 [==============================] - 0s 27us/sample - loss: 0.3060 - acc: 0.8818\n",
      "Epoch 29/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.3018 - acc: 0.8874\n",
      "Epoch 30/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.2965 - acc: 0.8870\n",
      "Epoch 31/50\n",
      "7265/7265 [==============================] - 0s 32us/sample - loss: 0.2919 - acc: 0.8858\n",
      "Epoch 32/50\n",
      "7265/7265 [==============================] - 0s 34us/sample - loss: 0.2880 - acc: 0.8889\n",
      "Epoch 33/50\n",
      "7265/7265 [==============================] - 0s 33us/sample - loss: 0.2843 - acc: 0.8914\n",
      "Epoch 34/50\n",
      "7265/7265 [==============================] - 0s 35us/sample - loss: 0.2788 - acc: 0.8933\n",
      "Epoch 35/50\n",
      "7265/7265 [==============================] - 0s 33us/sample - loss: 0.2730 - acc: 0.8953\n",
      "Epoch 36/50\n",
      "7265/7265 [==============================] - 0s 33us/sample - loss: 0.2695 - acc: 0.8968\n",
      "Epoch 37/50\n",
      "7265/7265 [==============================] - 0s 29us/sample - loss: 0.2647 - acc: 0.9017\n",
      "Epoch 38/50\n",
      "7265/7265 [==============================] - 0s 29us/sample - loss: 0.2613 - acc: 0.8972\n",
      "Epoch 39/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.2571 - acc: 0.9046\n",
      "Epoch 40/50\n",
      "7265/7265 [==============================] - 0s 29us/sample - loss: 0.2533 - acc: 0.9050\n",
      "Epoch 41/50\n",
      "7265/7265 [==============================] - 0s 27us/sample - loss: 0.2479 - acc: 0.9081\n",
      "Epoch 42/50\n",
      "7265/7265 [==============================] - 0s 29us/sample - loss: 0.2456 - acc: 0.9090\n",
      "Epoch 43/50\n",
      "7265/7265 [==============================] - 0s 27us/sample - loss: 0.2398 - acc: 0.9120\n",
      "Epoch 44/50\n",
      "7265/7265 [==============================] - 0s 30us/sample - loss: 0.2329 - acc: 0.9145\n",
      "Epoch 45/50\n",
      "7265/7265 [==============================] - 0s 30us/sample - loss: 0.2283 - acc: 0.9160\n",
      "Epoch 46/50\n",
      "7265/7265 [==============================] - 0s 36us/sample - loss: 0.2257 - acc: 0.9149\n",
      "Epoch 47/50\n",
      "7265/7265 [==============================] - 0s 27us/sample - loss: 0.2213 - acc: 0.9213\n",
      "Epoch 48/50\n",
      "7265/7265 [==============================] - 0s 27us/sample - loss: 0.2172 - acc: 0.9195\n",
      "Epoch 49/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.2158 - acc: 0.9203\n",
      "Epoch 50/50\n",
      "7265/7265 [==============================] - 0s 28us/sample - loss: 0.2089 - acc: 0.9247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11aa7d7b8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5085, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(768,)),\n",
    "        tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "LR = 1e-4\n",
    "model.compile(\n",
    "    tf.keras.optimizers.Adam(lr=LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5812 samples, validate on 1453 samples\n",
      "Epoch 1/50\n",
      "5812/5812 [==============================] - 0s 57us/sample - loss: 0.8199 - acc: 0.6564 - val_loss: 0.6618 - val_acc: 0.7330\n",
      "Epoch 2/50\n",
      "5812/5812 [==============================] - 0s 34us/sample - loss: 0.6417 - acc: 0.7326 - val_loss: 0.5987 - val_acc: 0.7522\n",
      "Epoch 3/50\n",
      "5812/5812 [==============================] - 0s 34us/sample - loss: 0.5943 - acc: 0.7497 - val_loss: 0.5794 - val_acc: 0.7571\n",
      "Epoch 4/50\n",
      "5812/5812 [==============================] - 0s 32us/sample - loss: 0.5722 - acc: 0.7591 - val_loss: 0.5633 - val_acc: 0.7674\n",
      "Epoch 5/50\n",
      "5812/5812 [==============================] - 0s 34us/sample - loss: 0.5539 - acc: 0.7669 - val_loss: 0.5523 - val_acc: 0.7729\n",
      "Epoch 6/50\n",
      "5812/5812 [==============================] - 0s 34us/sample - loss: 0.5424 - acc: 0.7713 - val_loss: 0.5480 - val_acc: 0.7708\n",
      "Epoch 7/50\n",
      "5812/5812 [==============================] - 0s 34us/sample - loss: 0.5331 - acc: 0.7756 - val_loss: 0.5429 - val_acc: 0.7722\n",
      "Epoch 8/50\n",
      "5812/5812 [==============================] - 0s 34us/sample - loss: 0.5245 - acc: 0.7784 - val_loss: 0.5365 - val_acc: 0.7722\n",
      "Epoch 9/50\n",
      "5812/5812 [==============================] - 0s 35us/sample - loss: 0.5194 - acc: 0.7810 - val_loss: 0.5378 - val_acc: 0.7688\n",
      "Epoch 10/50\n",
      "5812/5812 [==============================] - 0s 36us/sample - loss: 0.5105 - acc: 0.7832 - val_loss: 0.5380 - val_acc: 0.7736\n",
      "Epoch 11/50\n",
      "5812/5812 [==============================] - 0s 37us/sample - loss: 0.5061 - acc: 0.7863 - val_loss: 0.5391 - val_acc: 0.7763\n",
      "Epoch 12/50\n",
      "5812/5812 [==============================] - 0s 37us/sample - loss: 0.4994 - acc: 0.7887 - val_loss: 0.5313 - val_acc: 0.7743\n",
      "Epoch 13/50\n",
      "5812/5812 [==============================] - 0s 34us/sample - loss: 0.4931 - acc: 0.7942 - val_loss: 0.5316 - val_acc: 0.7756\n",
      "Epoch 14/50\n",
      "5812/5812 [==============================] - 0s 35us/sample - loss: 0.4889 - acc: 0.7944 - val_loss: 0.5303 - val_acc: 0.7729\n",
      "Epoch 15/50\n",
      "5812/5812 [==============================] - 0s 35us/sample - loss: 0.4856 - acc: 0.7997 - val_loss: 0.5284 - val_acc: 0.7777\n",
      "Epoch 16/50\n",
      "5812/5812 [==============================] - 0s 33us/sample - loss: 0.4791 - acc: 0.7989 - val_loss: 0.5338 - val_acc: 0.7756\n",
      "Epoch 17/50\n",
      "5812/5812 [==============================] - 0s 34us/sample - loss: 0.4752 - acc: 0.8035 - val_loss: 0.5313 - val_acc: 0.7722\n",
      "Epoch 18/50\n",
      "5812/5812 [==============================] - 0s 36us/sample - loss: 0.4675 - acc: 0.8057 - val_loss: 0.5329 - val_acc: 0.7805\n",
      "Epoch 19/50\n",
      "5812/5812 [==============================] - 0s 35us/sample - loss: 0.4649 - acc: 0.8059 - val_loss: 0.5320 - val_acc: 0.7777\n",
      "Epoch 20/50\n",
      "5812/5812 [==============================] - 0s 33us/sample - loss: 0.4600 - acc: 0.8097 - val_loss: 0.5303 - val_acc: 0.7736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11aa65860>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/Test_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yitinglin/Projects/news-emotion-analysis/env/lib/python3.6/site-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=512\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "for content in test_df.content.values:\n",
    "    test_data.append(bert_client.encode([str(content)])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7356, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7356, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59444314 0.40047696 0.00507991]\n",
      " [0.00951869 0.44488338 0.545598  ]\n",
      " [0.00357998 0.7644521  0.23196794]\n",
      " ...\n",
      " [0.5376648  0.4588728  0.00346242]\n",
      " [0.6087117  0.3863722  0.00491608]\n",
      " [0.01484168 0.5477541  0.43740413]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame()\n",
    "submission_df['id'] = test_df['id'].values\n",
    "submission_df['label'] = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_data).to_csv('./test_word_vector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5812, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape((64, 12), input_shape=(768,)),\n",
    "        tf.keras.layers.Conv1D(32, 8, activation=tf.nn.relu, input_shape=(64, 12)),\n",
    "        tf.keras.layers.Conv1D(32, 8, activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling1D(3),\n",
    "        tf.keras.layers.Conv1D(64, 8, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv1D(64, 8, activation=tf.nn.relu),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "LR = 1e-4\n",
    "model.compile(\n",
    "    tf.keras.optimizers.Adam(lr=LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[f1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5812 samples, validate on 1453 samples\n",
      "Epoch 1/50\n",
      "5812/5812 [==============================] - 1s 231us/sample - loss: 0.8741 - f1: 0.4506 - val_loss: 0.7513 - val_f1: 0.6775\n",
      "Epoch 2/50\n",
      "5812/5812 [==============================] - 1s 164us/sample - loss: 0.7081 - f1: 0.7020 - val_loss: 0.6660 - val_f1: 0.7230\n",
      "Epoch 3/50\n",
      "5812/5812 [==============================] - 1s 166us/sample - loss: 0.6520 - f1: 0.7260 - val_loss: 0.6271 - val_f1: 0.7350\n",
      "Epoch 4/50\n",
      "5812/5812 [==============================] - 1s 165us/sample - loss: 0.6160 - f1: 0.7382 - val_loss: 0.6023 - val_f1: 0.7392\n",
      "Epoch 5/50\n",
      "5812/5812 [==============================] - 1s 169us/sample - loss: 0.5889 - f1: 0.7479 - val_loss: 0.5825 - val_f1: 0.7461\n",
      "Epoch 6/50\n",
      "5812/5812 [==============================] - 1s 175us/sample - loss: 0.5695 - f1: 0.7560 - val_loss: 0.5883 - val_f1: 0.7485\n",
      "Epoch 7/50\n",
      "5812/5812 [==============================] - 1s 173us/sample - loss: 0.5586 - f1: 0.7617 - val_loss: 0.5626 - val_f1: 0.7546\n",
      "Epoch 8/50\n",
      "5812/5812 [==============================] - 1s 173us/sample - loss: 0.5455 - f1: 0.7710 - val_loss: 0.5809 - val_f1: 0.7374\n",
      "Epoch 9/50\n",
      "5812/5812 [==============================] - 1s 169us/sample - loss: 0.5412 - f1: 0.7715 - val_loss: 0.5479 - val_f1: 0.7598\n",
      "Epoch 10/50\n",
      "5812/5812 [==============================] - 1s 175us/sample - loss: 0.5303 - f1: 0.7791 - val_loss: 0.5624 - val_f1: 0.7491\n",
      "Epoch 11/50\n",
      "5812/5812 [==============================] - 1s 168us/sample - loss: 0.5259 - f1: 0.7804 - val_loss: 0.5485 - val_f1: 0.7591\n",
      "Epoch 12/50\n",
      "5812/5812 [==============================] - 1s 180us/sample - loss: 0.5179 - f1: 0.7803 - val_loss: 0.5405 - val_f1: 0.7635\n",
      "Epoch 13/50\n",
      "5812/5812 [==============================] - 1s 177us/sample - loss: 0.5108 - f1: 0.7848 - val_loss: 0.5400 - val_f1: 0.7603\n",
      "Epoch 14/50\n",
      "5812/5812 [==============================] - 1s 173us/sample - loss: 0.5034 - f1: 0.7894 - val_loss: 0.5379 - val_f1: 0.7662\n",
      "Epoch 15/50\n",
      "5812/5812 [==============================] - 1s 171us/sample - loss: 0.4982 - f1: 0.7929 - val_loss: 0.5383 - val_f1: 0.7671\n",
      "Epoch 16/50\n",
      "5812/5812 [==============================] - 1s 173us/sample - loss: 0.4921 - f1: 0.7977 - val_loss: 0.5349 - val_f1: 0.7655\n",
      "Epoch 17/50\n",
      "5812/5812 [==============================] - 1s 175us/sample - loss: 0.4859 - f1: 0.7991 - val_loss: 0.5519 - val_f1: 0.7512\n",
      "Epoch 18/50\n",
      "5812/5812 [==============================] - 1s 183us/sample - loss: 0.4809 - f1: 0.8040 - val_loss: 0.5350 - val_f1: 0.7684\n",
      "Epoch 19/50\n",
      "5812/5812 [==============================] - 1s 175us/sample - loss: 0.4805 - f1: 0.8056 - val_loss: 0.5399 - val_f1: 0.7651\n",
      "Epoch 20/50\n",
      "5812/5812 [==============================] - 1s 168us/sample - loss: 0.4715 - f1: 0.8067 - val_loss: 0.5400 - val_f1: 0.7626\n",
      "Epoch 21/50\n",
      "5812/5812 [==============================] - 1s 174us/sample - loss: 0.4672 - f1: 0.8105 - val_loss: 0.5439 - val_f1: 0.7649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1472e8550>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4720546 , 0.51232606, 0.01561937],\n",
       "       [0.03073281, 0.5135095 , 0.45575768],\n",
       "       [0.00545582, 0.8614308 , 0.1331134 ],\n",
       "       ...,\n",
       "       [0.57019615, 0.42523158, 0.0045723 ],\n",
       "       [0.76545376, 0.2305294 , 0.00401679],\n",
       "       [0.03027752, 0.62384385, 0.34587863]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame()\n",
    "submission_df['id'] = test_df['id'].values\n",
    "submission_df['label'] = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
